# -*- coding: utf-8 -*-
"""nlpLab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bsgPSCVxF7PhM2FQNIvgfbezhFm-hiUv
"""

#1a to read a paragraph from the user. Decompose it into sentences and words.

import re

text = input("Enter paragraph: ")

sentences = re.split(r'[.!?]', text)
words = re.findall(r'\w+', text)

print("Sentences:", sentences)
print("Words:", words)

#1b to read a paragraph from a file. Decompose it into sentences and words.

import re

file = open("input.txt","r")
text = file.read()

sentences = re.split(r'[.!?]', text)
words = re.findall(r'\w+', text)

print("Sentences:", sentences)
print("Words:", words)

#2 Implement a program in Python to find the number of occurrence for a given word using regular expression.

import re

text = input("Enter paragraph: ")
word = input("Enter word: ")

pattern = r'\b' + re.escape(word) + r'\b'
count = len(re.findall(pattern, text, re.IGNORECASE))

print("Count:", count)

#3 Develop a program in python to remove the stop words using NLTK.

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

text = input("Enter sentence: ")
words = word_tokenize(text)

stop = set(stopwords.words('english'))
result = [w for w in words if w.lower() not in stop]

print(result)

#4 Develop a program in python to remove the stop words without using NLTK.

stop = ['is','a','the','in','and','to','of']

text = input("Enter sentence: ")
words = text.split()

res = [w for w in words if w.lower() not in stop]
print(res)

#5 Accept a passage from the user. Analyse the words in the passage to identify the sentiment described in the passage.

pos = ['good','happy','nice']
neg = ['bad','sad','worst']

text = input("Enter text: ").lower().split()

score = 0
for w in text:
    if w in pos: score+=1
    if w in neg: score-=1

print("Positive" if score>0 else "Negative")

#6 Program to create a list with n values. The list of integers with exactly two occurrences of nineteen and at least three occurrences of five.

import random as rd

n = int(input("Enter list size (>=5): "))

nums = [19, 19, 5, 5, 5]                 # Step 1: Start with exactly 2×19 and 3×5

for _ in range(n - len(nums)):
    num = rd.randint(0, 20)                 # random number can be anything (including 5), but NOT 19
    while num == 19:                                       # ensure no extra 19s
        num = rd.randint(0, 20)
    nums.append(num)

rd.shuffle(nums)                # Step 3: Shuffle the list so numbers are in random positions

print("Generated list:", nums)
print("Count of 19:", nums.count(19))  # should be exactly 2
print("Count of 5:", nums.count(5))    # should be >=3

#7 7. Write a Python program to implement the following algorithms for stemming and analyse the results obtained:
#a. Porter's Stemmer

from nltk.stem import PorterStemmer

ps = PorterStemmer()
words = ["running", "studies", "easily"]

for w in words:
 print(w, "->", ps.stem(w))

#b. Snowball Stemmer

from nltk.stem import SnowballStemmer

stemmer = SnowballStemmer("english")

words = ["running", "studies", "easily"]

for w in words:
    print(w, "->", stemmer.stem(w))

#c. Lancaster Stemmer

from nltk.stem import LancasterStemmer

stemmer = LancasterStemmer()

words = ["running", "studies", "easily"]

for w in words:
    print(w, "->", stemmer.stem(w))

#d. Regexp Stemmer

from nltk.stem import RegexpStemmer

# Remove common suffixes: ing, ly, es, s, ed
stemmer = RegexpStemmer('ing$|ly$|es$|s$|ed$', min=1)

words = ["running", "studies", "easily"]

for w in words:
    print(w, "->", stemmer.stem(w))

#8 Develop a python program to lemmatize the words in a sentence using NLTK:
#a. Without using Part-of-Speech tagging

from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

sentence = input("Enter a sentence: ")
words = sentence.lower().split()

print("Lemmatized words:")
for w in words:
    print(w, "->", lemmatizer.lemmatize(w))

#8 b. Using Part-of-Speech tagging

from nltk.stem import WordNetLemmatizer
from nltk import pos_tag, word_tokenize
from nltk.corpus import wordnet
import nltk

#nltk.download('all')

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger_eng') # Changed to download 'averaged_perceptron_tagger_eng'
nltk.download('wordnet')
nltk.download('omw-1.4')

lemmatizer = WordNetLemmatizer()

def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

sentence = input("Enter a sentence: ")
words = word_tokenize(sentence)
pos_tags = pos_tag(words)

print("Lemmatized words with POS:")
for word, tag in pos_tags:
    wn_tag = get_wordnet_pos(tag)
    print(word, "->", lemmatizer.lemmatize(word, wn_tag))

#9 Develop a program to implement the minimum edit distance for converting source string to the target string following the dynamic programming approach given the cost of insertion as 1, cost of deletion as 1

def min_edit_distance(source, target):
    m, n = len(source), len(target)

    # Create DP table
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    # Initialization
    for i in range(m + 1):
        dp[i][0] = i        # deletion
    for j in range(n + 1):
        dp[0][j] = j        # insertion

    # Fill DP table
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if source[i - 1] == target[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = min(
                    dp[i - 1][j] + 1,      # deletion
                    dp[i][j - 1] + 1,      # insertion
                    dp[i - 1][j - 1] + 1   # substitution
                )

    return dp[m][n]


# Input
s = input("Enter source string: ")
t = input("Enter target string: ")

# Output
print("Minimum Edit Distance:", min_edit_distance(s, t))

#10. Create a collection of 3 documents and a program to find the frequency of bi-grams in this collection. Find the number of bi-grams that occur and display the top 5 most common.


from nltk import bigrams, word_tokenize
from collections import Counter
import nltk

nltk.download('punkt')

docs = [
    "Natural language processing is interesting",
    "Language processing is a part of artificial intelligence",
    "Natural language understanding and processing"
]

bg = []

for doc in docs:
    words = word_tokenize(doc.lower())
    bg += list(bigrams(words))  # only within this doc

freq = Counter(bg)   # Count frequency

print("Total unique bi-grams:", len(freq))
print("Top 5 most common bi-grams:")
for b, c in freq.most_common(5):
    print(b, ":", c)

#11. Develop a program in python using NLTK to calculate the probability of the occurrence of a sentence from the given corpus.
#Bigram

import nltk
from nltk import word_tokenize, bigrams
from collections import Counter

nltk.download('punkt')

# Corpus
corpus = """Natural language processing is interesting.
Language processing is a part of artificial intelligence.
Natural language understanding and processing."""

# Tokenize words and remove punctuation
words = [w for w in word_tokenize(corpus.lower()) if w.isalpha()]

# Count unigrams and bigrams
uni_freq = Counter(words)
bi_freq = Counter(bigrams(words))

# Input sentence
sentence = input("Enter a sentence: ").lower()
s_words = [w for w in word_tokenize(sentence) if w.isalpha()]

# Bigram probability
prob = 1.0
for w1, w2 in bigrams(s_words):
    prob *= bi_freq.get((w1, w2), 0) / uni_freq.get(w1, 1)  # avoid division by 0

print("Probability of the sentence:", prob)

#12 Write a program to extract all the noun phrases from a text file.

import nltk
from nltk import pos_tag, word_tokenize, RegexpParser

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

with open("input.txt", "r") as f:
    text = f.read()

sentences = nltk.sent_tokenize(text)

grammar = "NP: {<DT>?<JJ>*<NN.*>+}"   # Define a simple noun phrase grammar: optional adjectives + noun

cp = RegexpParser(grammar)  # Create a parser

noun_phrases = []  # Extract noun phrases

for sent in sentences:
    words = word_tokenize(sent)
    tags = pos_tag(words)
    tree = cp.parse(tags)

    for subtree in tree.subtrees():
        if subtree.label() == 'NP':
            np = " ".join(word for word, tag in subtree.leaves())     # Join words to make the phrase
            noun_phrases.append(np)

print("Noun Phrases:")
for np in noun_phrases:
    print(np)

